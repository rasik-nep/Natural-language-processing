{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyON5cIw1W1gUEad4EdJZb6w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rasik-nep/Natural-language-processing-/blob/main/TextPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "-1w29gfesAS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization refers to the process of converting a sequence of text into smaller parts, known as tokens. These tokens can be as small as characters or as long as words.\n",
        "\n",
        "\n",
        "*   Corpus: Body of text\n",
        "*   Lexicon: Words and their meanings\n",
        "*   Token : Each “entity” that is a part of whatever was split up based on rules\n",
        "\n"
      ],
      "metadata": {
        "id": "HNQnbdP9r89V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlyNNOYGqq5K",
        "outputId": "02454716-07bc-4869-ebf6-0300202a95bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zbDvdvtq7yJ",
        "outputId": "625f21b5-cceb-4ad7-e4e0-e3c706e87cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy text\n",
        "text =\"\"\"\n",
        "    Imagine you're trying to teach a child to read. Instead of diving straight into complex paragraphs,\n",
        "    you'd start by introducing them to individual letters, then syllables, and finally, whole words.\n",
        "    In a similar vein, tokenization breaks down vast stretches of text into more digestible and understandable\n",
        "    units for machines. The primary goal of tokenization is to represent text in a manner that's meaningful for\n",
        "    machines without losing its context. By converting text into tokens, algorithms can more easily identify patterns.\n",
        "    This pattern recognition is crucial because it makes it possible for machines to understand and respond to human input.\n",
        "    For instance, when a machine encounters the word \"running\", it doesn't see it as a singular entity but rather\n",
        "    as a combination of tokens that it can analyze and derive meaning from.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gvzSSragtOzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKjCfMPStjFE",
        "outputId": "55285f1c-5e51-4d85-dcb9-761399340cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"\\n    Imagine you're trying to teach a child to read.\", \"Instead of diving straight into complex paragraphs,\\n    you'd start by introducing them to individual letters, then syllables, and finally, whole words.\", 'In a similar vein, tokenization breaks down vast stretches of text into more digestible and understandable\\n    units for machines.', \"The primary goal of tokenization is to represent text in a manner that's meaningful for\\n    machines without losing its context.\", 'By converting text into tokens, algorithms can more easily identify patterns.', 'This pattern recognition is crucial because it makes it possible for machines to understand and respond to human input.', 'For instance, when a machine encounters the word \"running\", it doesn\\'t see it as a singular entity but rather\\n    as a combination of tokens that it can analyze and derive meaning from.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing words\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTJlJ590uYML",
        "outputId": "e5a573cf-ea7e-479a-e823-b5532f8768a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Imagine', 'you', \"'re\", 'trying', 'to', 'teach', 'a', 'child', 'to', 'read', '.', 'Instead', 'of', 'diving', 'straight', 'into', 'complex', 'paragraphs', ',', 'you', \"'d\", 'start', 'by', 'introducing', 'them', 'to', 'individual', 'letters', ',', 'then', 'syllables', ',', 'and', 'finally', ',', 'whole', 'words', '.', 'In', 'a', 'similar', 'vein', ',', 'tokenization', 'breaks', 'down', 'vast', 'stretches', 'of', 'text', 'into', 'more', 'digestible', 'and', 'understandable', 'units', 'for', 'machines', '.', 'The', 'primary', 'goal', 'of', 'tokenization', 'is', 'to', 'represent', 'text', 'in', 'a', 'manner', 'that', \"'s\", 'meaningful', 'for', 'machines', 'without', 'losing', 'its', 'context', '.', 'By', 'converting', 'text', 'into', 'tokens', ',', 'algorithms', 'can', 'more', 'easily', 'identify', 'patterns', '.', 'This', 'pattern', 'recognition', 'is', 'crucial', 'because', 'it', 'makes', 'it', 'possible', 'for', 'machines', 'to', 'understand', 'and', 'respond', 'to', 'human', 'input', '.', 'For', 'instance', ',', 'when', 'a', 'machine', 'encounters', 'the', 'word', '``', 'running', \"''\", ',', 'it', 'does', \"n't\", 'see', 'it', 'as', 'a', 'singular', 'entity', 'but', 'rather', 'as', 'a', 'combination', 'of', 'tokens', 'that', 'it', 'can', 'analyze', 'and', 'derive', 'meaning', 'from', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "lfLHfsFmwmQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming, in Natural Language Processing (NLP), refers to the process of reducing a word to its word stem that affixes to suffixes and prefixes or the roots.\n",
        "For example, the stem of the words “eating,” “eats,” “eaten” is “eat.”\n",
        "There are different stemming alogrithms. Here we are using Porter Stemmer algo."
      ],
      "metadata": {
        "id": "82pMsPvbwqjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WW4PsfWw6e2",
        "outputId": "06a7619c-8ab2-4c6a-9557-dcf5933fe193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating object for PortStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "B7gxqLcJxMOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords does not add much value to the overall context. So, we can remove them."
      ],
      "metadata": {
        "id": "LJIp-E5lyku9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0B8IuxrxPkt",
        "outputId": "8232d55b-e163-435b-9f43-09c1b2f7526b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"imagin 're tri teach child read .\", \"instead dive straight complex paragraph , 'd start introduc individu letter , syllabl , final , whole word .\", 'similar vein , token break vast stretch text digest understand unit machin .', \"primari goal token repr text manner 's mean machin without lose context .\", 'convert text token , algorithm easili identifi pattern .', 'thi pattern recognit crucial make possibl machin understand respond human input .', \"instanc , machin encount word `` run `` , n't see singular entiti rather combin token analyz deriv mean .\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem with stemming is that it produces intermediate representation of word which may not have any meaning."
      ],
      "metadata": {
        "id": "AW84WYcuzRUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "Ddcc_CXZ0uRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is the process of grouping together different inflected forms of the same word.\n",
        "Lemmatization takes a word and breaks it down to its lemma. For example, the verb \"walk\" might appear as \"walking,\" \"walks\" or \"walked.\" Inflectional endings such as \"s,\" \"ed\" and \"ing\" are removed. Lemmatization groups these words as its lemma, \"walk.\""
      ],
      "metadata": {
        "id": "mc7oVtRC1ACH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEZJYJvO0yqv",
        "outputId": "1c299ee4-3f9f-4800-f6db-e182fc6a86f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy text\n",
        "text =\"\"\"\n",
        "    Imagine you're trying to teach a child to read. Instead of diving straight into complex paragraphs,\n",
        "    you'd start by introducing them to individual letters, then syllables, and finally, whole words.\n",
        "    In a similar vein, tokenization breaks down vast stretches of text into more digestible and understandable\n",
        "    units for machines. The primary goal of tokenization is to represent text in a manner that's meaningful for\n",
        "    machines without losing its context. By converting text into tokens, algorithms can more easily identify patterns.\n",
        "    This pattern recognition is crucial because it makes it possible for machines to understand and respond to human input.\n",
        "    For instance, when a machine encounters the word \"running\", it doesn't see it as a singular entity but rather\n",
        "    as a combination of tokens that it can analyze and derive meaning from.\n",
        "\"\"\"\n",
        "sentences = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "M3NaoId_1BAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "GSTZt1qI1JYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBUsMY8W1QPw",
        "outputId": "a1439b4f-5c60-494d-f2eb-5f1708583bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Imagine 're trying teach child read .\", \"Instead diving straight complex paragraph , 'd start introducing individual letter , syllable , finally , whole word .\", 'In similar vein , tokenization break vast stretch text digestible understandable unit machine .', \"The primary goal tokenization represent text manner 's meaningful machine without losing context .\", 'By converting text token , algorithm easily identify pattern .', 'This pattern recognition crucial make possible machine understand respond human input .', \"For instance , machine encounter word `` running '' , n't see singular entity rather combination token analyze derive meaning .\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "pUtuwH_rNCgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A popular and simple method of feature extraction with text data.\n",
        "This model is a simple document embedding technique based on word frequency.\n"
      ],
      "metadata": {
        "id": "hjJ-bvAkNHmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"\"\"\n",
        "Nepal officially the Federal Democratic Republic of Nepal is a landlocked central Himalayan country in South Asia. Nepal is divided into 7 states and 77 districts and 753 local units including 6 metropolises, 11 sub-metropolises, 246 municipal councils and 481 villages. It has a population of 26.4 million and is the 93rd largest country by area. Bordering China in the north and India in the south, east, and west, it is the largest sovereign Himalayan state.  Nepal has a diverse geography, including fertile plains, subalpine forested hills, and eight of the world’s ten tallest mountains, including Mount Everest, the highest point on Earth. Kathmandu is the nation’s capital and largest city.\n",
        "Nepal is a place of festivals . Festivals may be linked with the remembrance of the departed soul, to herald the different seasons, to mark the beginning or end of the agricultural cycle, to mark the national events, or just family celebrations.  On a festive day the Nepalese take their ritual bath, worship different gods and goddesses, visit temple, observe fasting and undertake feasting.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "U9NkjbB5Nr23"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the text\n",
        "# Making all the words in lower case\n",
        "# Also tokenization, Stemming and/or lemmatization is done\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "corpus=[]"
      ],
      "metadata": {
        "id": "KUJeDMJ-NH9Y"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[^a-zA-Z]',' ', sentences[i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  review = [stemmer.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKzD6Fc5Oio9",
        "outputId": "5d5fe289-da33-401e-f7de-809d1ede4184"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nepal offici feder democrat republ nepal landlock central himalayan countri south asia', 'nepal divid state district local unit includ metropolis sub metropolis municip council villag', 'popul million rd largest countri area', 'border china north india south east west largest sovereign himalayan state', 'nepal divers geographi includ fertil plain subalpin forest hill eight world ten tallest mountain includ mount everest highest point earth', 'kathmandu nation capit largest citi', 'nepal place festiv', 'festiv may link remembr depart soul herald differ season mark begin end agricultur cycl mark nation event famili celebr', 'festiv day nepales take ritual bath worship differ god goddess visit templ observ fast undertak feast']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[^a-zA-Z]',' ', sentences[i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQyxIfjjSnPf",
        "outputId": "f6235df6-d1ba-4289-93b1-bd0c355af935"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nepal officially federal democratic republic nepal landlocked central himalayan country south asia', 'nepal divided state district local unit including metropolis sub metropolis municipal council village', 'population million rd largest country area', 'bordering china north india south east west largest sovereign himalayan state', 'nepal diverse geography including fertile plain subalpine forested hill eight world ten tallest mountain including mount everest highest point earth', 'kathmandu nation capital largest city', 'nepal place festival', 'festival may linked remembrance departed soul herald different season mark beginning end agricultural cycle mark national event family celebration', 'festive day nepalese take ritual bath worship different god goddess visit temple observe fasting undertake feasting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall sklearn -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHGSRQ2RTEfv",
        "outputId": "20ddf60f-8bfb-4de3-9700-fdc9d3df55d7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping sklearn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the bag of words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WybM5QWkS_Jf",
        "outputId": "e24fdb2d-5568-4160-e120-f7f1c6b412a5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
            "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 2 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1\n",
            "  0 0 0 1 1 0 2 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 1 0 2 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
            "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disadvantage : All the words have the same weight so it is not suitable for Sentiment analysis"
      ],
      "metadata": {
        "id": "YpDA80rGUdHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "IOUgD-AhV4r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
        "\n",
        "This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents."
      ],
      "metadata": {
        "id": "lBMfyB5oV9pU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean the text\n",
        "# lemmatization\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[^a-zA-Z]',' ', sentences[i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU6zrc0rg4Ky",
        "outputId": "b8bd9c27-9e15-4990-afb4-95bd52c5d664"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nepal officially federal democratic republic nepal landlocked central himalayan country south asia', 'nepal divided state district local unit including metropolis sub metropolis municipal council village', 'population million rd largest country area', 'bordering china north india south east west largest sovereign himalayan state', 'nepal diverse geography including fertile plain subalpine forested hill eight world ten tallest mountain including mount everest highest point earth', 'kathmandu nation capital largest city', 'nepal place festival', 'festival may linked remembrance departed soul herald different season mark beginning end agricultural cycle mark national event family celebration', 'festive day nepalese take ritual bath worship different god goddess visit temple observe fasting undertake feasting', 'nepal officially federal democratic republic nepal landlocked central himalayan country south asia', 'nepal divided state district local unit including metropolis sub metropolis municipal council village', 'population million rd largest country area', 'bordering china north india south east west largest sovereign himalayan state', 'nepal diverse geography including fertile plain subalpine forested hill eight world ten tallest mountain including mount everest highest point earth', 'kathmandu nation capital largest city', 'nepal place festival', 'festival may linked remembrance departed soul herald different season mark beginning end agricultural cycle mark national event family celebration', 'festive day nepalese take ritual bath worship different god goddess visit temple observe fasting undertake feasting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the TF-IDF model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rkG5A7ehIU6",
        "outputId": "84bacdfa-2220-43bf-de90-dac760cb67b1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.30820435 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.43995275 0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.22169494 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.25259275]]\n"
          ]
        }
      ]
    }
  ]
}