{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+VCWpGC2i4WBGeMcsf8Zd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rasik-nep/Natural-language-processing-/blob/main/TextPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "-1w29gfesAS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization refers to the process of converting a sequence of text into smaller parts, known as tokens. These tokens can be as small as characters or as long as words.\n",
        "\n",
        "\n",
        "*   Corpus: Body of text\n",
        "*   Lexicon: Words and their meanings\n",
        "*   Token : Each “entity” that is a part of whatever was split up based on rules\n",
        "\n"
      ],
      "metadata": {
        "id": "HNQnbdP9r89V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlyNNOYGqq5K",
        "outputId": "02454716-07bc-4869-ebf6-0300202a95bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zbDvdvtq7yJ",
        "outputId": "625f21b5-cceb-4ad7-e4e0-e3c706e87cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy text\n",
        "text =\"\"\"\n",
        "    Imagine you're trying to teach a child to read. Instead of diving straight into complex paragraphs,\n",
        "    you'd start by introducing them to individual letters, then syllables, and finally, whole words.\n",
        "    In a similar vein, tokenization breaks down vast stretches of text into more digestible and understandable\n",
        "    units for machines. The primary goal of tokenization is to represent text in a manner that's meaningful for\n",
        "    machines without losing its context. By converting text into tokens, algorithms can more easily identify patterns.\n",
        "    This pattern recognition is crucial because it makes it possible for machines to understand and respond to human input.\n",
        "    For instance, when a machine encounters the word \"running\", it doesn't see it as a singular entity but rather\n",
        "    as a combination of tokens that it can analyze and derive meaning from.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gvzSSragtOzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKjCfMPStjFE",
        "outputId": "55285f1c-5e51-4d85-dcb9-761399340cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"\\n    Imagine you're trying to teach a child to read.\", \"Instead of diving straight into complex paragraphs,\\n    you'd start by introducing them to individual letters, then syllables, and finally, whole words.\", 'In a similar vein, tokenization breaks down vast stretches of text into more digestible and understandable\\n    units for machines.', \"The primary goal of tokenization is to represent text in a manner that's meaningful for\\n    machines without losing its context.\", 'By converting text into tokens, algorithms can more easily identify patterns.', 'This pattern recognition is crucial because it makes it possible for machines to understand and respond to human input.', 'For instance, when a machine encounters the word \"running\", it doesn\\'t see it as a singular entity but rather\\n    as a combination of tokens that it can analyze and derive meaning from.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing words\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTJlJ590uYML",
        "outputId": "e5a573cf-ea7e-479a-e823-b5532f8768a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Imagine', 'you', \"'re\", 'trying', 'to', 'teach', 'a', 'child', 'to', 'read', '.', 'Instead', 'of', 'diving', 'straight', 'into', 'complex', 'paragraphs', ',', 'you', \"'d\", 'start', 'by', 'introducing', 'them', 'to', 'individual', 'letters', ',', 'then', 'syllables', ',', 'and', 'finally', ',', 'whole', 'words', '.', 'In', 'a', 'similar', 'vein', ',', 'tokenization', 'breaks', 'down', 'vast', 'stretches', 'of', 'text', 'into', 'more', 'digestible', 'and', 'understandable', 'units', 'for', 'machines', '.', 'The', 'primary', 'goal', 'of', 'tokenization', 'is', 'to', 'represent', 'text', 'in', 'a', 'manner', 'that', \"'s\", 'meaningful', 'for', 'machines', 'without', 'losing', 'its', 'context', '.', 'By', 'converting', 'text', 'into', 'tokens', ',', 'algorithms', 'can', 'more', 'easily', 'identify', 'patterns', '.', 'This', 'pattern', 'recognition', 'is', 'crucial', 'because', 'it', 'makes', 'it', 'possible', 'for', 'machines', 'to', 'understand', 'and', 'respond', 'to', 'human', 'input', '.', 'For', 'instance', ',', 'when', 'a', 'machine', 'encounters', 'the', 'word', '``', 'running', \"''\", ',', 'it', 'does', \"n't\", 'see', 'it', 'as', 'a', 'singular', 'entity', 'but', 'rather', 'as', 'a', 'combination', 'of', 'tokens', 'that', 'it', 'can', 'analyze', 'and', 'derive', 'meaning', 'from', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "lfLHfsFmwmQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming, in Natural Language Processing (NLP), refers to the process of reducing a word to its word stem that affixes to suffixes and prefixes or the roots.\n",
        "For example, the stem of the words “eating,” “eats,” “eaten” is “eat.”\n",
        "There are different stemming alogrithms. Here we are using Porter Stemmer algo."
      ],
      "metadata": {
        "id": "82pMsPvbwqjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WW4PsfWw6e2",
        "outputId": "06a7619c-8ab2-4c6a-9557-dcf5933fe193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating object for PortStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "B7gxqLcJxMOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords does not add much value to the overall context. So, we can remove them."
      ],
      "metadata": {
        "id": "LJIp-E5lyku9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0B8IuxrxPkt",
        "outputId": "8232d55b-e163-435b-9f43-09c1b2f7526b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"imagin 're tri teach child read .\", \"instead dive straight complex paragraph , 'd start introduc individu letter , syllabl , final , whole word .\", 'similar vein , token break vast stretch text digest understand unit machin .', \"primari goal token repr text manner 's mean machin without lose context .\", 'convert text token , algorithm easili identifi pattern .', 'thi pattern recognit crucial make possibl machin understand respond human input .', \"instanc , machin encount word `` run `` , n't see singular entiti rather combin token analyz deriv mean .\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem with stemming is that it produces intermediate representation of word which may not have any meaning."
      ],
      "metadata": {
        "id": "AW84WYcuzRUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "Ddcc_CXZ0uRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is the process of grouping together different inflected forms of the same word.\n",
        "Lemmatization takes a word and breaks it down to its lemma. For example, the verb \"walk\" might appear as \"walking,\" \"walks\" or \"walked.\" Inflectional endings such as \"s,\" \"ed\" and \"ing\" are removed. Lemmatization groups these words as its lemma, \"walk.\""
      ],
      "metadata": {
        "id": "mc7oVtRC1ACH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEZJYJvO0yqv",
        "outputId": "1c299ee4-3f9f-4800-f6db-e182fc6a86f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy text\n",
        "text =\"\"\"\n",
        "    Imagine you're trying to teach a child to read. Instead of diving straight into complex paragraphs,\n",
        "    you'd start by introducing them to individual letters, then syllables, and finally, whole words.\n",
        "    In a similar vein, tokenization breaks down vast stretches of text into more digestible and understandable\n",
        "    units for machines. The primary goal of tokenization is to represent text in a manner that's meaningful for\n",
        "    machines without losing its context. By converting text into tokens, algorithms can more easily identify patterns.\n",
        "    This pattern recognition is crucial because it makes it possible for machines to understand and respond to human input.\n",
        "    For instance, when a machine encounters the word \"running\", it doesn't see it as a singular entity but rather\n",
        "    as a combination of tokens that it can analyze and derive meaning from.\n",
        "\"\"\"\n",
        "sentences = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "M3NaoId_1BAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "GSTZt1qI1JYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(words)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBUsMY8W1QPw",
        "outputId": "a1439b4f-5c60-494d-f2eb-5f1708583bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Imagine 're trying teach child read .\", \"Instead diving straight complex paragraph , 'd start introducing individual letter , syllable , finally , whole word .\", 'In similar vein , tokenization break vast stretch text digestible understandable unit machine .', \"The primary goal tokenization represent text manner 's meaningful machine without losing context .\", 'By converting text token , algorithm easily identify pattern .', 'This pattern recognition crucial make possible machine understand respond human input .', \"For instance , machine encounter word `` running '' , n't see singular entity rather combination token analyze derive meaning .\"]\n"
          ]
        }
      ]
    }
  ]
}